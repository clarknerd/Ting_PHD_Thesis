% !TEX root = ../paper.tex
Automated analysis of database access logs is critical for solving a wide range of problems, from database performance tuning~\cite{DBLP:conf/sigmod/BrunoC05}, to compliance validation~\cite{DBLP:conf/icalp/Dwork06}, and query recommendation~\cite{DBLP:journals/debu/ChatzopoulouEKMPV11}. 
For example, the Peloton self-tuning database~\cite{DBLP:conf/cidr/PavloAALLMMMPQS17} searches for optimal configurations by repeatedly simulating database performance based on statistical properties of historical queries.
Unfortunately, query logs for production databases can grow to be large ---
A recent study of queries at a major US bank for a period of 19 hours found nearly 17 million SQL queries and over 60 million stored procedure executions~\cite{DBLP:conf/www/KulLXCCKU16} --- and computing these properties from the log itself is slow.

Tracking only a sample of these queries is not sufficient, as rare queries can disproportionately affect database performance, for example, if they benefit from an otherwise unnecessary index.
Rather, we need a compressed \emph{summary} of the log on which we can compute aggregate statistical properties.
The problems of compression and summarization have been studied extensively (e.g.,~\cite{DBLP:journals/tit/ZivL77,DBLP:journals/tit/ZivL78,4051119,lee1999learning,DBLP:reference/stat/Jolliffe11,DBLP:journals/cacm/Blei12,DBLP:conf/acl/WangZLG09,DBLP:journals/ai/KnightM02}). 
However, these schemes either require the use of heavyweight inference to desired statistical measures, or produce unnecessarily large encodings.

In this chapter, we adapt ideas from pattern mining and summarization~\cite{DBLP:journals/tkdd/MampaeyVT12,DBLP:journals/pvldb/GebalyAGKS14} to propose a middle-ground: \systemnameone, a summarization scheme that facilitates efficient (both in terms of storage and time) approximation of workload statistics.
By adjusting a tunable parameter in \systemnameone, users can choose to obtain a high-fidelity, albeit large summary, or obtain a more compact summary with lower fidelity.
Constructing the summary that best balances compactness and fidelity is challenging, as the search space of candidate summaries is combinatorially large~\cite{DBLP:journals/tkdd/MampaeyVT12,DBLP:journals/pvldb/GebalyAGKS14}.
\systemnameone offers a new approach to summary construction that avoids searching this space, making inexpensive, accurate computation of aggregate workload statistics possible.
As a secondary benefit, the resulting summaries are also human-interpretable.

\systemnameone does not admit closed-form solutions to classical fidelity measures like information loss, so an alternative called \emph{\errorname} is proposed.
A combination of analytical and experimental evidence will be shown that \errorname is highly correlated with several classical measures of encoding fidelity.

\systemnameone-compressed data relies on a codebook of structural elements like \texttt{SELECT} items, \texttt{FROM} tables, or conjunctive \texttt{WHERE} clauses~\cite{DBLP:journals/kais/AligonGMRT14}.
This codebook provides a bi-directional mapping from SQL queries to a bit-vector encoding, reducing the compression problem to one of compactly encoding a collection of feature-vectors.
The problem is further simplified by observing that a common theme in use cases like automated performance tuning or query recommendation is the need for predominantly aggregate workload statistics.
As these are order-independent, we are able to focus exclusively on compactly representing \emph{bags} of feature-vectors.

\systemnameone works by identifying groups of co-occurring structural elements that we call patterns.  
A family of \emph{pattern encodings} of access logs is defined, which map patterns to their frequencies in the log.
For pattern encodings, two idealized measures of fidelity are considered: 
(1) Ambiguity, which measures how much room the encoding leaves for interpretation; and 
(2) Deviation, which measures how reliably the encoding approximates the original log.
Neither Ambiguity nor Deviation can be computed efficiently for pattern encodings.
Hence a measure called \emph{\errorname} is proposed that is efficiently computable and that closely tracks both Ambiguity and Deviation.

In general, the size of the encoding is inversely related with \errorname: The more detailed the encoding, the more faithfully it represents the original log.
Thus, log compression may be defined as a search over the space of pattern-based encodings to identify the one that best trades off between these two properties.
Unfortunately, searching for such an ideal encoding from the space can be computationally expensive~\cite{DBLP:journals/tkdd/MampaeyVT12,DBLP:journals/pvldb/GebalyAGKS14}.
To overcome this limitation, the search space is reduced by first clustering entries in the log and then encoding each cluster separately, an approach that we call \textit{pattern mixture encoding}.
Finally I identify a simple approach for encoding individual clusters that we call \textit{naive mixture encodings}, and show experimentally that it produces results competitive with more general techniques for log compression and summarization.

Concretely, in this chapter I make the following contributions:
(1) I define two families of compression for query logs: pattern and pattern mixture, 
(2) I define a computationally efficient measure, \errorname, and demonstrate that it is a close approximation of Ambiguity and Deviation (two commonly used measures),
(3) I propose a clustering-based approach to efficiently search for naive mixture encodings, and show how these encodings can be further optimized, and, 
(4) I experimentally validate \systemnameone and show that it produces more precise encodings, faster than several state-of-the-art pattern encoding algorithms.

\tinysection{Roadmap}
This chapter is organized as follows: Section~\ref{sec:problemdefinition} formally defines the log compression problem and the summary representation;
Section~\ref{sec:analyzingsummaries} then defines information loss of the summaries;
Section~\ref{sec:practicalrepresentativeness} explains the difficulty in computing classical loss measures and provides a practical alternative;
Section~\ref{sec:patternmixtureencodings} motivates data partitioning and generalizes the practical loss measure to partitioned data;
Section~\ref{sec:constructingencodings} then introduces the proposed \systemnameone compression scheme;
Section~\ref{sec:experiments} empirically validates the practical loss measure and evaluates the effectiveness of \systemnameone by comparing it with two state-of-the-art summarization methods;
Section~\ref{sec:evaluatingalternativeapplications} further evaluates \systemnameone under applications of comparison methods.