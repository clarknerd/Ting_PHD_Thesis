\appendix
\section{NOMENCLATURE}
 \begin{tabular}{|c c|} 
 \hline
 \textbf{Symbol} & \textbf{Meaning}  \\ [0.5ex] 
 \hline\hline
 $f$ & Feature\\ 
 \hline
 $\vec{b}$ & Pattern \\
 \hline
  $\vec{b'} \subseteq \vec{b}$ & $\vec{b'}$ is contained in $\vec{b}$\\
 \hline  
 $\vec{q}$ & Query\\
 \hline
 $L$ & Log, a bag of queries\\
  \hline
 $Q$ & Query randomly drawn from $L$\\
 \hline 
  $p(Q\;|\;L)$ & Query distribution of $L$\\
 \hline
 $p(Q\supseteq\vec{b}\;|\;L)$ & Marginal probability of $Q\supseteq\vec b$\\
 \hline
  $\encoding_L[\pattern]$ & same as  $p(Q\supseteq\vec{b}\;|\;L)$\\
 \hline
   $corr\_rank(\vec{b})$ & Feature-correlation score\\ 
 \hline 
  $\encoding_{max}$ & Mapping from all patterns to marginals\\
 \hline 
  $\encoding$ & Encoding, partial mapping $\encoding \subseteq \encoding_{max}$\\
  \hline 
  $domain(\cdot)$ & Domain of mapping\\
 \hline
  $\rho$ & An arbitrary query distribution\\
 \hline
  $\Omega_\encoding$ & Space of $\rho$ constrained by $\encoding$\\
 \hline
   $\encoding\leq_{\Omega}\encoding'$ & $\Omega_{\encoding}\subseteq\Omega_{\encoding'}$\\ 
   \hline
   $\mathcal{P}_{\encoding}$ & A random $\rho$ drawn from $\Omega_\encoding$\\
  \hline 
  $\rho^*$ & Same as $p(Q\;|\;L)$, $\rho^*\in\Omega_\encoding$\\
 \hline 
    $\mathcal{H}(\cdot)$ & Entropy of Distribution\\
 \hline 
  $\overline{\rho}_\encoding$ & Representative distribution of $\Omega_\encoding$\\
 \hline 
 $\text{d}(\encoding)$ & Deviation\\
 \hline 
  $\text{I}(\encoding)$ & Ambiguity\\
 \hline 
  $e(\encoding)$ & \errorname \\
 \hline
   $|\encoding|$ & $|domain(\encoding)|$, Verbosity of encoding\\
   \hline
  $\rho\ll\rho'$ & $\rho$ is absolutely continuous w.r.t $\rho'$\\
 \hline 
 $\mathcal{D}_{KL}(\rho||\rho')$ & K-L Divergence from $\rho'$ to $\rho$.\\
 \hline
\end{tabular}

\section{Proof of Proposition~\ref{proposition:losslesssummary}}
\label{appendix:losslesssummary}
Denote by $\mathds{1}^n = \{0,1\}^n$ the space of possible 0-1 vectors of size $n$, and define an encoding $\bar \encoding_{\vec{q}}$ with patterns:
$$domain(\overline{\encoding}_{\vec{q}})=\comprehension{(x_1+b_1,\ldots,x_n+b_n)}{(b_1,\ldots,b_n)\in\mathds{1}^n}$$
We will show that $\overline{\encoding}_{\vec{q}}\subseteq \encoding_{max}$ contains sufficient information to compute $p_0 = p(X_1=x_1,\ldots,X_n=x_n)$ through several steps.
First, we define a new pair of marginal probabilities $p_1\tuple{b_1} = p(X_1 \geq x_1 + b_1,X_2=x_2,\ldots,X_n=x_n)$.
$x_1$ is integral, so $p_0 = p_1\tuple{0} - p_1\tuple{1}$.
Generalizing, we can define:
\begin{multline*}
p_k\tuple{b_1, \ldots, b_k} = p(X_1 \geq x_1 + b_1,\; \ldots,\; X_k \geq x_k + b_k, \\
  X_{k+1} = x_{k+1},\; \ldots,\; X_n=x_n)
\end{multline*}
Again, $x_k$ being integral gives us that:
\begin{multline*}
p_{k-1}\tuple{b_1, \ldots, b_{k-1}} = p_{k}\tuple{b_1, \ldots, b_{k-1}, 0} \\
  -p_{k}\tuple{b_1, \ldots, b_{k-1}, 1}
\end{multline*}
Finally, when $k = n$, the probability $p_n\tuple{b_1, \ldots, b_n}$ is the marginal probability $p(Q\supseteq\vec{b}\;|\;L)$ of a pattern $\vec{b}=(x_1+b_1, \ldots, x_n+b_n)$, which by definition is offered by $\overline{\encoding}_{\vec{q}}$ for any $(b_1, \ldots, b_n) \in \mathds{1}^n$.
The resulting encoding $\overline{\encoding}=\bigcup_{\vec{q}\in L}\overline{\encoding}_{\vec{q}}$ identifies the distribution $p(Q\;|\;L)$, which we refer to as \emph{lossless} encoding. Clearly any encoding that extends $\overline{\encoding}$ (including $\encoding_{max}$) is lossless.

\section{Marginal Selection}
\label{appendix:marginalselectionforpatternbasedsummary}
As discussed in Section~\ref{sec:lossysummaries}, a pattern $\vec b$ conveys a piece of information content of the log by constraining an arbitrary query distribution $\rho$ towards the true distribution $p(Q\;|\;L)$ by aligning its marginal of pattern $\vec b$ with the true marginal $p(Q\supseteq\vec{b}\;|\;L)$. 
However, marginals based on relationship $Q\supseteq\vec b$ are not the only choice for carrying information content of the log. 
In fact, the marginal probability that a query $\vec{q}$ uniformly drawn from the log \textit{exactly matches} (denoted as $\vec{q}\sqsupseteq\vec{b}$) the pattern $\vec{b}$ can be an alternative. Specifically, given query $\vec{q}=(x_1,\ldots,x_n)$ and pattern $\vec{b}=(x_1',\ldots,x_n')$, 
$$\vec{q}\sqsupseteq\vec{b}\equiv\forall x_i'>0,\;x_i=x_i'$$
We explain the rationale behind choosing $p(Q\supseteq\vec{b})$ over $p(Q\sqsupseteq\vec{b})$ in the following example.
\begin{example}
Objective: Conveying the piece of information content that $i$th and $j$th features never co-occur together in any query drawn from the log.
Denote occurrences of $i$th feature as variable $X_i$.

Two Solutions: 
\begin{enumerate}
\item Marginal $p(Q\supseteq\vec{b})$: $p(X_i\geq 1,X_j\geq 1)=0$

\item Marginal $p(Q\sqsupseteq\vec{b})$:
$p(X_i=1,X_j=1)=0$,

$p(X_i=1+1,X_j=1)=0$,

$p(X_i=1,X_j=1+1)=0$,

\ldots

We are able to reach the objective using only one pattern with marginal $p(Q\supseteq\vec{b})$. 
On the contrary, it requires infinitely more patterns with marginal $p(Q\sqsupseteq\vec{b})$.
Marginal $p(Q\supseteq\vec{b})$ offers more descriptive power than $p(Q\sqsupseteq\vec{b})$.
\end{enumerate}
\end{example}

\section{Sampling From Space of Distributions}
\label{appendix:sampling}
Here we describe how we sample from the space $\Omega_\encoding$ of probability distributions.
\subsection{Preliminary Sampling}
To sample a random distribution $\rho : \mathbb{N}^n\to[0,1]$ without any constraint, the naive way is to treat $\rho$ as a multi-dimensional vector $(\rho(\vec{q}_1),\ldots,\rho(\vec{q}_{|\mathbb{N}^n|}))$ drawn from the vector space $\mathbb{R}^{|\mathbb{N}^n|}$. Note that it is actually a subspace of $\mathbb{R}^{|\mathbb{N}^n|}$ as the vector need to sum up to 1.
\begin{algorithm}
\caption{Sampling}
\label{alg:sampling}
\begin{algorithmic}[1]
\Procedure{TwoStepSampling}{}
\label{procedure:twostepsampling}
\State Step 1:
\For{each $\vec{v}\in \mathbb{B}^m\wedge\mathcal{C}_{\vec{v}}\neq\emptyset$}
\State $V \gets V\; \bigcup \;\vec{v}$
\EndFor
\State $class\_p \gets $\Call{UniRandDistribProb}{V,$1$}
\State Step 2:
\For{each $\vec{v}\in V$}
\State $\rho \gets \rho\; \bigcup \; $\Call{UniRandDistribProb}{$\mathcal{C}_{\vec{v}}$,$class\_p(\vec{v})$}
\EndFor
\State \Return $\rho$
\EndProcedure

\State

\Procedure{UniRandDistribProb}{Set S, double prob}
\For{each element $e \in S$}
\State $p(e) \gets UniformRandNum(range=[0,1])$
\EndFor
\For{each element $e \in S$}
\State $p(e) \gets prob\times p(e)\div\mysuml_{e}p(e)$
\EndFor
\State \Return $p$
\EndProcedure
\end{algorithmic}
\end{algorithm}
However, $|\mathbb{N}^n|$ is too large and we reduce the number of dimensions by grouping them (i.e., $\vec{q}_1\ldots,\vec{q}_{|\mathbb{N}^n|}$) into equivalence classes.

\tinysection{Encoding-equivalent Classes}
The basic idea for grouping is based on containment relationship between query $\vec q_i$ and patterns $\vec{b}\in \encoding$ in the encoding $\encoding$. 
More precisely, if $\vec{q}_i\supseteq\vec{b}$, it indicates that the assignment $\rho(\vec{q}_i)$ on $i$th dimension is constrained by marginal $S[\vec{b}]$ (See Equation~\ref{equation:constraints} in Section~\ref{sec:lossysummaries}).
As a result, if queries $\vec{q}_i,\vec{q}_j$ share the same containment relationship with pattern $\vec{b}$, assignments $\rho(\vec{q}_i),\rho(\vec{q}_j)$ on $i$th and $j$th dimension make no difference for satisfying the constraint of pattern $\vec{b}$.
We thus define \textit{pattern-equivalence} as $$\vec{q}_i\equiv_{\vec{b}}\vec{q}_j\Leftrightarrow BI(\vec{q}_i,\vec{b})=BI(\vec{q}_j,\vec{b})$$ 
$BI(\vec{q}_i,\vec{b})$ is the Binary Indicator function satisfying $BI(\vec{q}_i,\vec{b})=1\equiv\vec{q}_i\supseteq\vec{b}$. 
Queries are \textit{encoding-equivalent} $\vec{q}_i\equiv_{\encoding}\vec{q}_j$ if they are pattern-equivalent for all patterns in the encoding. 
Numbering patterns in the encoding as $\vec{b}_1,\ldots,\vec{b}_m$, any binary vector $\vec{v}\in\mathbb{B}^m$ maps to an equivalence class $\mathcal{C}_{\vec{v}}=\comprehension{\vec{q}}{(BI(\vec{q},\vec{b}_1),\ldots,BI(\vec{q},\vec{b}_m))=\vec{v}\wedge\vec{q}\in\mathbb{N}^n}$.
Though the number of non-empty equivalent classes may grow as large as $\mathcal{O}(2^m)$, it is much smaller than $|\mathbb{N}^n|$ and sampling a random distribution $\rho$ can be divided into two steps as shown in line~\ref{procedure:twostepsampling} of algorithm~\ref{alg:sampling}. 
Note that $class\_p$ in the algorithm, which is produced by the first step, is a randomly sampled distribution over all non-empty equivalence classes. The second step redistributes probabilities randomly assigned to each equivalence class to its class members in an unbiased way.

\subsection{Incorporating Constraints}
So far we are creating random samples from an unconstrained space of distributions. 
To make sure $\rho$ produced by the two-step sampling fall within space $\Omega_\encoding$, distribution $class\_p$ over equivalence classes must obey the linear equality constraints derived from the encoding $\encoding$.
Denote the space of all possible $class\_p$ as $U$ and the subspace allowed by the encoding as $U_\encoding\subseteq U$, one naive solution is to reject $class\_p\notin U_\encoding$. 
However, the subspace $U_\encoding$ constrained under linear equality constraints (See Section~\ref{sec:lossysummaries}) is equivalent to an intersection of \textit{hyperplanes} in the full space $U$. 
The volume of $U_\encoding$ is thus so small comparing to that of $U$, such that any random sample $class\_p\in U$ will \textit{almost never} fall within $U_\encoding$.
To make sampling feasible, we do not reject a sample $class\_p\in U$ but \textit{project} it onto $U_\encoding$ by finding its \textit{closest} counterpart $class\_p'\in U_\encoding$:
$$proj(class\_p)=\argminl_{class\_p'\in U_\encoding}d(class\_p',class\_p)$$
Function $d(\cdot\; , \;\cdot)$ represents some distance measure, e.g., $d(class\_p,class\_p')=||class\_p'-class\_p||$. 
In other words, given encoding $\encoding$, we assume we are sampling $class\_p'$ from allowed subspace $U_\encoding$ given the following probability measure $$p(class\_p')\propto\setsize{\comprehension{class\_p}{class\_p\in U\;\wedge\;proj(class\_p)=class\_p'}}$$
The projection can be achieved by solving an minimization problem under linear constraints.

\section{Algorithm Configurations}
\label{appendix:experimentsettingsforpatternbasedalgorithms}
Here we give detailed description on our selected state-of-the-art pattern based summarizers (i.e., \textit{Laserlight} and \textit{MTV}) and also specify how we configured them in experiments discussed in Section~\ref{sec:motivatepatternmixturesummaries}. 

\tinysection{Common Configuration}
We set up both algorithms to mine 12 patterns from target clusters.
This is because, empirically we found that mining over $12$ patterns has high chance of getting duplicates for both \textit{MTV} and \textit{Laserlight}. 
Since we are comparing them with naive mixture encoding based on running time, we try to avoid underestimating their computational efficiency by mining duplicate patterns.

\subsection{Laserlight Algorithm}
\label{appendix:Laserlight}
\tinysection{Description}
\textit{Laserlight} algorithm is proposed in~\cite{ElGebaly:2014:IIE:2735461.2735467} for summarizing multi-dimension data (i.e., $D=(X_1,\ldots,X_n)$) augmented by a binary attribute $A$. 
The goal is to search for a set of patterns (i.e., encoding) from the data $D$ that provide maximum information for predicting augmented attribute $A$, which is a sub-problem of summarizing the joint distribution $p(D,A)$. 
Another algorithm \textit{Flashlight} is also proposed in the same paper but we omit it in our experiment due its inferior scalability.
The implementation of \textit{Laserlight} has been incorporated into PostgreSQL 9.1 and the source code is only available upon request.

\tinysection{Experiment Settings}
Due to the restriction on the maximum number of data dimension by $Laserlight$, we project the distribution $p(Q\;|\;L)$ onto a limited set of $100$ features.
The selection criteria is based on entropy. 
More precisely, regarding the occurrence of $i$th feature as random variable $X_i$ with probability measure $p(X_i=x_i)$, features are ranked by entropy $\entropy(X_i)$. 
The feature with binary occurrence that has highest entropy $\entropy(X_i)$ is chosen as the augmented attribute $A$.
The algorithm heuristically selects a limited set of samples from the space of candidate patterns, from which the pattern that is most informative (See the definition of informative in the paper) is selected to be added to the encoding.
When we applied $Laserlight$ in our experiments, we set the number of samples to be $16$, which is suggested in the paper.

\subsection{MTV Algorithm}
\tinysection{Description}
\textit{MTV} algorithm is short for \textit{Maximally informaTiVe}, proposed in~\cite{Mampaey:2012:SDS:2382577.2382580} for summarizing multi-dimensional data with binary attributes. 
The goal is to mine a succinct set of patterns (i.e., encoding) that convey the most important information (See the paper for definition). 
The implementation of this algorithm can be obtained at \href{http://adrem.ua.ac.be/succinctsummary}{http://adrem.ua.ac.be/succinctsummary}.

\tinysection{Experiment Settings}
\textit{MTV} requires to set the minimum support threshold for patterns. That is, patterns with marginal less than the threshold will be ignored, in order to reduce the search space of candidate patterns.
We set the minimum support threshold to be $0.05$ in our experiments such that any pattern that is contained in more than $5\%$ of queries will be considered as candidate.

\subsection{Configuring Regularized Laserlight Mixture}
\label{appendix:Configuring_Regularized Laserlight_Mixture}
Given a data partitioning and fixed total number of patterns to mine from all clusters, to determine the number of patterns mined from each cluster, we need to assign weights $\sum_i w_i=1$ for each cluster $i$. 
\errorname $e(\encoding_L)$ of the naive encoding $\encoding_L$ for a cluster reflects its `hardness' for pattern mining and the intuition is that $e(\encoding_L)=0$ indicates there is no need for additional pattern mining.
\errorname $e(\encoding_L)$ is affected by the number of features $n$ ever occur in the cluster.
Consider a toy data with only two feature vectors $\vec v_1=(0,0)$ and $\vec v_2=(1,1)$. Appending $\vec v_1$ with new features of value $0$ and $\vec v_2$ of value $1$ will increase $e(\encoding_L)$ but not necessarily the number of patterns needed for accurately summarizing the data.
Hence we normalize $e(\encoding_L)$, dividing it by the number of features $n$, which gives us $w_i=\frac{e(\encoding_L)}{n}$.
In addition, since the \textit{generalized measure} for \textit{Laserlight} and \textit{MTV} gives weight to each cluster proportional to its number of distinct data instances $m$, we also adjust $w_i$ and multiply them with the number of distance data instances.
The final weight assignment becomes $w_i \propto \frac{e(\encoding_L)*m}{n}$.

\section{Interpreting Naive Mixture Encoding}
\label{appendix:naivemixturesummaryvisualization}
Due to sensitive information contained in the US bank data set, here we only provide visualization on PocketData data set.

The visualization of PocketData is based on its naive mixture encoding under $8$ clusters\footnote{The number of clusters is chosen for convenience of visualization.}. 
The result is given in Figure~\ref{fig:visualizepocketdatabyitsnaivemixturesummary}.
There are 5 sub-figures with each representing a naive encoding for one cluster. 
Note that we use shading to represent the magnitude of marginals and features with marginal too small will be invisible and omitted.
Question mark `$?$' is the placeholder for constants.
Three clusters from the eight are not shown in the figure: One cluster is too messy (i.e., further sub-clustering is needed) and two clusters gives similar visualization to Figure~\ref{fig:cluster1} and~\ref{fig:cluster5}.
The caption of each sub-figures expresses our understanding on the task that queries in the cluster are performing, by visualizing the corresponding naive encodings.
For simplicity, we will also omit features of \texttt{SELECT} category if they are neither participating in \texttt{WHERE} clause nor intuitively related to other features in \texttt{SELECT}. 
\begin{figure}
 \centering
\begin{subfigure}{\columnwidth}
  {\small
    \begin{tabular}{r|p{60mm}}
    \textbf{SELECT} & 
            {\texttt{conversation\_id}}, {\texttt{participants\_type}},
        {\texttt{first\_name}},
        {\texttt{chat\_id}},
        \texttt{blocked},
        \texttt{active}\\ \hline
    \textbf{FROM} &
        \texttt{{\texttt{conversation\_participants\_view}}}\\ \hline
    \textbf{WHERE} &
        \texttt{(chat\_id!=?)} $\wedge$
        {\texttt{(conversation\_id=?)}} $\wedge$
        \texttt{(active=1)}   
    \end{tabular}
  }
  \bfcaption{Check the person who is active in specific conversation and not participating in specified chat.}
  \label{fig:cluster1}
\end{subfigure}\\[2mm]

\begin{subfigure}{\columnwidth}
  {\small
    \begin{tabular}{r|p{60mm}}
    \textbf{SELECT} & 
            {\texttt{status}}, 
        {\texttt{timestamp}},
        {\texttt{expiration\_timestamp}},
        {\texttt{sms\_raw\_sender}},
        \texttt{message\_id},
        \texttt{text}\\ \hline
    \textbf{FROM} &
        \texttt{{\texttt{conversations}}},
        \textcolor{mid-gray}{\texttt{{\texttt{message\_notifications\_view}}}},
        \texttt{{\texttt{messages\_view}}}\\ \hline
       \textbf{ORDER BY} &
       \texttt{{\texttt{Descend on timestamp}}}
       \\ \hline
        \textbf{Limit} &
       \texttt{{\texttt{500}}}
       \\ \hline
    \textbf{WHERE} &      
        \textcolor{mid-gray}{\texttt{(expiration\_timestamp>?)}} $\wedge$
        \texttt{(status!=5)} $\wedge$
        {\texttt{(conversation\_id=?)}} $\wedge$        
{\texttt{(conversations.conversation\_id=conversation\_id)}}   
    \end{tabular}
  }
  \bfcaption{Check sender information for most recent SMS messages that participate in given conversation.}
  \label{fig:cluster2}
\end{subfigure}\\[2mm]

\begin{subfigure}{\columnwidth}
  {\small
    \begin{tabular}{r|p{60mm}}
    \textbf{SELECT} & 
            {\texttt{status}}, 
        {\texttt{timestamp}},
        {\texttt{conversation\_id}},
        {\texttt{chat\_watermark}},
        \texttt{message\_id},
        \texttt{sms\_type}\\ \hline
    \textbf{FROM} &
        \texttt{{\texttt{conversations}}},
        \texttt{{\texttt{message\_notifications\_view}}}\\ \hline
    \textbf{WHERE} &
        \texttt{(conversation\_status!=1)} $\wedge$
         \texttt{(conversation\_pending\_leave!=1)} $\wedge$
         \texttt{(conversation\_notification\_level!=10)} $\wedge$
         \texttt{(timestamp>1355...)} $\wedge$
         \texttt{(timestamp>chat\_watermark)} $\wedge$
        {\texttt{(conversation\_id=?)}} $\wedge$        
{\texttt{(conversations.conversation\_id=conversation\_id)}}   
    \end{tabular}
  }
  \bfcaption{Check recent messages in conversations of specific type.}
  \label{fig:cluster3}
\end{subfigure}\\[2mm]

\begin{subfigure}{\columnwidth}
  {\small
    \begin{tabular}{r|p{60mm}}
    \textbf{SELECT} & 
            {\texttt{suggestion\_type}}, {\texttt{name}},
        {\texttt{chat\_id}}\\ \hline
    \textbf{FROM} &
        \texttt{{\texttt{suggested\_contacts}}}\\ \hline
       \textbf{Limit} &
        \textcolor{light-gray}{\texttt{{\texttt{10}}}}\\ \hline
        \textbf{Order By} &
        \textcolor{light-gray}{\texttt{{\texttt{Ascend on upper(name)}}}}\\ \hline
    \textbf{WHERE} &
        \texttt{(chat\_id!=?)} $\wedge$
        {\texttt{(name!=?)}}
    \end{tabular}
  }
  \bfcaption{Suggest contacts that avoid certain names and chat.}
  \label{fig:cluster4}
\end{subfigure}\\[2mm]

\begin{subfigure}{\columnwidth}
  {\small
    \begin{tabular}{r|p{60mm}}
    \textbf{SELECT} & 
            \textcolor{mid-gray}{{\texttt{sms\_type}}},  \textcolor{mid-gray}{{\texttt{timestamp}}},
         \textcolor{mid-gray}{{\texttt{\_id}}}\\ \hline
    \textbf{FROM} &
        \texttt{{\texttt{messages}}}\\ \hline
    \textbf{WHERE} &
        \texttt{(sms\_type=1)} $\wedge$
        {\texttt{(status=4)}} $\wedge$
         {\texttt{(transport\_type=3)}} $\wedge$
         \textcolor{mid-gray}{{\texttt{(timestamp>=?)}}}
    \end{tabular}
  }
  \bfcaption{Check messages under type/status conditions}
  \label{fig:cluster5}
\end{subfigure}\\[2mm]

\bfcaption{\textbf{Visualize PocketData by its naive mixture encoding}}
\label{fig:visualizepocketdatabyitsnaivemixturesummary}
\trimfigurewhitespace
\end{figure}