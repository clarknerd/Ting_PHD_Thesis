% !TEX root = ../paper.tex
Our goal is to encode the distribution $p(Q)$ as a set of patterns: obtaining a less verbose encoding (i.e., with fewer patterns), while also ensuring that the encoding captures $p(Q)$ with minimal information loss.
In this section, we define information loss for pattern-based encodings.

\subsection{Lossless Summaries}
\label{sec:representativeness:idealdef}
To establish a baseline for measuring information loss, we begin with the extreme cases.
At one extreme, an empty encoding ($|\encoding| = 0$) conveys no information.
At the other extreme, we have the encoding $\encoding_{max}$ which is the full mapping from all patterns. 
Having this encoding is a sufficient condition to exactly reconstruct the original distribution $p(Q)$. 
\vspace*{-4mm}
\begin{proposition}
\label{PROPOSITION:LOSSLESSSUMMARY} 
For any query $\vec{q}=(x_1,\ldots,x_n)$ $\in{0,1}^n$,
the probability of drawing exactly $\vec{q}$ at random from the log (i.e., $p(X_1=x_1,\ldots,X_n=x_n)$) is computable, given $\encoding_{max}$.
\end{proposition}
%\begin{proof}
%\todo{See Appendix~\ref{appendix:losslesssummary} $\leftarrow$ this needs to be turned into a pointer to a tech report}
%\end{proof}

%%%%%%%%%%%%%%%%%%%%%%

\subsection{Lossy Summaries}
\label{sec:lossysummaries}
Although $\encoding_{max}$ is lossless, its Verbosity is exponential in the number of features ($n$). 
Hence, we will focus on lossy encodings that can be less verbose.
A lossy encoding $\encoding \subset \encoding_{max}$ may not precisely identify the distribution $p(Q)$, but can still be used to approximate it.
We characterize the information content of a lossy encoding $\encoding$ by defining a \emph{space} (denoted by $\Omega_\encoding$) of distributions $\rho \in \Omega_\encoding$ allowed by an encoding $\encoding$.
This space is defined by constraints as follows:
First, we have the general properties of probability distributions:
\begin{center}
$\forall \vec{q}\in\{0,1\}^n:\rho(\vec{q})\geq 0$
\hspace{10mm}
$\sum_{\vec{q}}\rho(\vec{q})=1$
\end{center}
Each pattern $\vec b$ in the encoding $\encoding$ constrains relevant probabilities in distribution $\rho$ to sum to the target frequency:
\begin{equation*}
\forall \vec{b} \in domain(\encoding)  :\;\; \encoding[\vec{b}] = \sum\nolimits_{\vec{q}\supseteq\vec{b}} \rho(\vec{q}) \;\;\;
\end{equation*}
Note that the dual constraints $1-\encoding[\vec{b}]=\sum_{\vec{q}\not\supseteq\vec{b}} \rho(\vec{q})$ are redundant under constraint $\sum_{\vec{q}}\rho(\vec{q})=1$.

The resulting space $\Omega_\encoding$ is the set of all query logs, or equivalently the set of all possible distributions of queries, that obey these constraints.
From the outside observer's perspective, the distribution $\rho\in\Omega_\encoding$ that the encoding conveys is ambiguous: We model this ambiguity using a random variable $\mathcal P_\encoding$ with support $\Omega_\encoding$.
The true distribution $p(Q)$ derived from the query log must appear in $\Omega_\encoding$, denoted as $p(Q)\equiv\rho^*\in\Omega_\encoding$ (i.e., $p(\mathcal P_\encoding = \rho^*) > 0$). 
Of the remaining distributions $\rho$ admitted by $\Omega_\encoding$, it is possible that some are more likely than others.
For example, a query containing a column (e.g., \texttt{status}) is only valid if it also references a table that contains the column (e.g., \texttt{Messages}).
This prior knowledge may be modeled as a prior on the distribution of $\mathcal P_\encoding$ or equivalently by an additional constraint.
However, for the purposes of this paper, we take the uninformed prior by assuming that $\mathcal P_\encoding$ is uniformly distributed over $\Omega_\encoding$:
\begin{equation*}
\label{uniformprior}
p(\mathcal P_\encoding = \rho) = 
\begin{cases}
\frac{1}{|\Omega_\encoding|} & \text{if } \rho \in \Omega_\encoding\\
0 & \text{otherwise}
\end{cases}
\end{equation*}

%------------------------------------------

\tinysection{Naive Encodings}
One specific family of lossy encodings that treat each feature as being independent (e.g., as in Figure~\ref{fig:screenshots:nocorrelation}) is of particular interest to us.  
We call this family \emph{naive encodings}, and return to it throughout the rest of the paper.
A naive encoding $\naiveencoding$ is composed of all patterns that have exactly one feature with non-zero frequency.
$$domain(\naiveencoding)=\comprehension{(0, \ldots, 0, x_i, 0, \ldots, 0)}{i \in [1,n],\; x_i = 1}$$ 


\subsection{Idealized Information Loss Measures}
\label{sec:idealizedrepresentativenessmeasures}
Based on the space of distributions constrained by the encoding, the information loss of an encoding can be considered from two related, but subtly distinct perspectives:
(1) \emph{Ambiguity} measures how much room the encoding leaves for interpretation and 
(2) \emph{Deviation} measures how reliably the encoding approximates the target distribution $p(Q)$.

\smallskip
\tinysection{Ambiguity}
We define the Ambiguity $\text{I}(\encoding)$ of an encoding as the entropy of the random variable $\mathcal P_\encoding$. 
The higher the entropy, the less precisely $\encoding$ identifies a specific distribution.% $\rho\in\Omega_\encoding$.
$$\text{I}(\encoding) = \mysuml_{\rho}p(\mathcal P_\encoding = \rho)\log \left(p(\mathcal P_\encoding=\rho)\right)$$

\tinysection{Deviation}
The deviation from any permitted distribution $\rho$ to the true distribution $\rho^*$ can be measured by the Kullback-Leibler (K-L) divergence $\mathcal{D}_{KL}(\rho^*||\rho)$.
We define the Deviation $\text{d}(\encoding)$ of a encoding as the expectation of the K-L divergence over all permitted $\rho \in \Omega_\encoding$:
$$\text{d}(\encoding)=\expect_{\mathcal{P}_\encoding}\left[\mathcal{D}_{KL}(\rho^*||\mathcal{P}_\encoding)\right] = \sum_{\rho \in \Omega_\encoding} p(\mathcal P_\encoding = \rho) \cdot \mathcal{D}_{KL}(\rho^*||\rho)$$

\tinysection{Limitations}
There are two limitations to these idealized measures in practice.
First, K-L divergence is not defined on any permitted distribution $\rho$ where the true distribution $\rho^*$ is not \emph{absolutely continuous} (denoted $\rho^*\ll\rho$). 

Second, neither Deviation nor Ambiguity has a closed-form formula. 

