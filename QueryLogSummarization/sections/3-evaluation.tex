% !TEX root = ../paper.tex

% Notes on things that need to be / can be added later
%   - Sub-patterns (i.e., skeletons)
%   - Restricted fragments of pattern based summaries.
%   - Task-specific options (non-uniform priors)

Our goal is to encode the distribution $p(Q)$ as a set of patterns: obtaining a less verbose encoding (i.e., with fewer patterns), while also ensuring that the encoding captures $p(Q)$ with minimal information loss.
In this section, we define information loss for pattern-based encodings.

\subsection{Lossless Summaries}
\label{sec:representativeness:idealdef}
To establish a baseline for measuring information loss, we begin with the extreme cases.
At one extreme, an empty encoding ($|\encoding| = 0$) conveys no information.
At the other extreme, we have the encoding $\encoding_{max}$ which is the full mapping from all patterns. 
Having this encoding is a sufficient condition to exactly reconstruct the original distribution $p(Q)$. 
\vspace*{-4mm}
\begin{proposition}
\label{PROPOSITION:LOSSLESSSUMMARY} 
For any query $\vec{q}=(x_1,\ldots,x_n)$ $\in{0,1}^n$,
the probability of drawing exactly $\vec{q}$ at random from the log (i.e., $p(X_1=x_1,\ldots,X_n=x_n)$) is computable, given $\encoding_{max}$.
\end{proposition}
%\begin{proof}
%\todo{See Appendix~\ref{appendix:losslesssummary} $\leftarrow$ this needs to be turned into a pointer to a tech report}
%\end{proof}

%%%%%%%%%%%%%%%%%%%%%%

\subsection{Lossy Summaries}
\label{sec:lossysummaries}
Although $\encoding_{max}$ is lossless, its Verbosity is exponential in the number of features ($n$). 
Hence, we will focus on lossy encodings that can be less verbose.
A lossy encoding $\encoding \subset \encoding_{max}$ may not precisely identify the distribution $p(Q)$, but can still be used to approximate it.
We characterize the information content of a lossy encoding $\encoding$ by defining a \emph{space} (denoted by $\Omega_\encoding$) of distributions $\rho \in \Omega_\encoding$ allowed by an encoding $\encoding$.
This space is defined by constraints as follows:
First, we have the general properties of probability distributions:
\begin{center}
$\forall \vec{q}\in\{0,1\}^n:\rho(\vec{q})\geq 0$
\hspace{10mm}
$\sum_{\vec{q}}\rho(\vec{q})=1$
\end{center}
Each pattern $\vec b$ in the encoding $\encoding$ constrains relevant probabilities in distribution $\rho$ to sum to the target frequency:
\begin{equation*}
\forall \vec{b} \in domain(\encoding)  :\;\; \encoding[\vec{b}] = \sum\nolimits_{\vec{q}\supseteq\vec{b}} \rho(\vec{q}) \;\;\;
\end{equation*}
Note that the dual constraints $1-\encoding[\vec{b}]=\sum_{\vec{q}\not\supseteq\vec{b}} \rho(\vec{q})$ are redundant under constraint $\sum_{\vec{q}}\rho(\vec{q})=1$.

The resulting space $\Omega_\encoding$ is the set of all query logs, or equivalently the set of all possible distributions of queries, that obey these constraints.
From the outside observer's perspective, the distribution $\rho\in\Omega_\encoding$ that the encoding conveys is ambiguous: We model this ambiguity using a random variable $\mathcal P_\encoding$ with support $\Omega_\encoding$.
The true distribution $p(Q)$ derived from the query log must appear in $\Omega_\encoding$, denoted as $p(Q)\equiv\rho^*\in\Omega_\encoding$ (i.e., $p(\mathcal P_\encoding = \rho^*) > 0$). 
Of the remaining distributions $\rho$ admitted by $\Omega_\encoding$, it is possible that some are more likely than others.
For example, a query containing a column (e.g., \texttt{status}) is only valid if it also references a table that contains the column (e.g., \texttt{Messages}).
This prior knowledge may be modeled as a prior on the distribution of $\mathcal P_\encoding$ or equivalently by an additional constraint.
However, for the purposes of this paper, we take the uninformed prior by assuming that $\mathcal P_\encoding$ is uniformly distributed over $\Omega_\encoding$:
\begin{equation*}
\label{uniformprior}
p(\mathcal P_\encoding = \rho) = 
\begin{cases}
\frac{1}{|\Omega_\encoding|} & \text{if } \rho \in \Omega_\encoding\\
0 & \text{otherwise}
\end{cases}
\end{equation*}

%------------------------------------------

\tinysection{Naive Encodings}
One specific family of lossy encodings that treat each feature as being independent (e.g., as in Figure~\ref{fig:screenshots:nocorrelation}) is of particular interest to us.  
We call this family \emph{naive encodings}, and return to it throughout the rest of the paper.
A naive encoding $\naiveencoding$ is composed of all patterns that have exactly one feature with non-zero frequency.
$$domain(\naiveencoding)=\comprehension{(0, \ldots, 0, x_i, 0, \ldots, 0)}{i \in [1,n],\; x_i = 1}$$
% where $K_i$ is the lowest integer such that $p(X_i \geq K_i) = 0$.
% In other words, a naive encoding encodes the log as a sequence of independent categorical distributions:
% $$X_i \sim \operatorname{\mathit{Bernoulli(\theta_i)}}$$ 
% where $\theta_{i} \in [0,1]$ is the probability of a query drawn uniformly at random from the log having feature $i$.

% Naive encoding is an instance of a more general probabilistic generative model (see Section~\ref{sec:backgroundandrelatedwork}).
% More precisely, under specific valuation of its parameters $\Theta$ (i.e., single-feature marginals), log $L$ or equivalently distribution $p(Q)$ can be re-produced with some likelihood $p(L\;|\tuple{M,\Theta})$.
% Interpreting parameters of a naive encoding is straightforward, while the interpretability is not guaranteed for more sophisticated models.

%------------------------------------------


\subsection{Idealized Information Loss Measures}
\label{sec:idealizedrepresentativenessmeasures}
Based on the space of distributions constrained by the encoding, the information loss of an encoding can be considered from two related, but subtly distinct perspectives:
(1) \emph{Ambiguity} measures how much room the encoding leaves for interpretation and 
(2) \emph{Deviation} measures how reliably the encoding approximates the target distribution $p(Q)$.

\smallskip
\tinysection{Ambiguity}
We define the Ambiguity $\text{I}(\encoding)$ of an encoding as the entropy of the random variable $\mathcal P_\encoding$. 
The higher the entropy, the less precisely $\encoding$ identifies a specific distribution.% $\rho\in\Omega_\encoding$.
$$\text{I}(\encoding) = \mysuml_{\rho}p(\mathcal P_\encoding = \rho)\log \left(p(\mathcal P_\encoding=\rho)\right)$$

\tinysection{Deviation}
The deviation from any permitted distribution $\rho$ to the true distribution $\rho^*$ can be measured by the Kullback-Leibler (K-L) divergence $\mathcal{D}_{KL}(\rho^*||\rho)$.
We define the Deviation $\text{d}(\encoding)$ of a encoding as the expectation of the K-L divergence over all permitted $\rho \in \Omega_\encoding$:
$$\text{d}(\encoding)=\expect_{\mathcal{P}_\encoding}\left[\mathcal{D}_{KL}(\rho^*||\mathcal{P}_\encoding)\right] = \sum_{\rho \in \Omega_\encoding} p(\mathcal P_\encoding = \rho) \cdot \mathcal{D}_{KL}(\rho^*||\rho)$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Information Loss for Probabilistic Models}
% Ambiguity and Deviation, which are based on the world of possible logs $\mathcal{P}_\encoding$, or equivalently the space $\Omega_\encoding$ can be generalized to probabilistic models. 
% A probabilistic model $M$ with parameters $\Theta$ is also known as \textit{generative} as it can also produce a world of possible logs $\mathcal{P}_{\tuple{M,\Theta}}$ or equivalently a space $\Omega_{\tuple{M,\Theta}}$ of possible distributions $\rho$ with $p(\mathcal{P}_{M,\Theta}=\rho)=p(\rho\;|\tuple{M,\Theta})$.

\tinysection{Limitations}
There are two limitations to these idealized measures in practice.
First, K-L divergence is not defined on any permitted distribution $\rho$ where the true distribution $\rho^*$ is not \emph{absolutely continuous} (denoted $\rho^*\ll\rho$). 
%In other words, Deviation is only defined on variable $\hat{\mathcal{P}}$ with regulated probability measure $\neg(\rho^*\ll\rho)\to P(\hat{\mathcal{P}}=\rho)=0$.
Second, neither Deviation nor Ambiguity has a closed-form formula. 
% We introduce a practical form of information loss in next section that subsumes both idealized measures.
