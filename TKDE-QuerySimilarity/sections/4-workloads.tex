%!TEX root = ../paper.tex
This section introduces the quality measures and workloads to evaluate three query similarity metrics and the feature engineering scheme.

Our goal is to evaluate how well a query similarity metric captures the task behind a query with and without regularization.
Two types of real-world query workloads are used: human- and machine-generated.  
The problem of query similarity is expected to be harder on human-generated workloads, as queries generated by machines are more likely to follow a strict, rigid structural pattern.

As a source of human-generated queries, two different sets of student answers to database course assignments are used.
Many database courses include homework or exam questions where students are asked to translate prose into a precise SQL query.  
This provides us with a ground-truth source of queries with different structures that should be similar.

As machine-generated queries, PocketData~\cite{pocketdata} is used which is a log of 33 million queries issued by smartphone apps running on 11 phones in the wild over the course of a month.

In subsection~\ref{subsec:data}, the datasets used are outlined.
Then, in subsection~\ref{subsec:validation}, the experimental methodology used to evaluate distance metrics are outlined, and a set of measures are proposed for quantitatively assessing how effective a query similarity metric is at clustering queries with similar tasks.

\subsection{Workloads}
\label{subsec:data}
Three specific query sets are used: Student assignments gathered by IIT Bombay~\cite{chandra2015Data}, student exams gathered at UB CSE department (denoted as UB dataset in the experiments) and released in~\ref{8352666}\footnote{\url{http://odin.cse.buffalo.edu/public_data/2016-UB-Exam-Queries.zip}
}, and SQL query logs of the Google+ app extracted from PocketData dataset~\cite{pocketdata}.

The first dataset~\cite{chandra2015Data} consists of student answers to SQL questions given in IIT Bombay's undergraduate databases course.
The dataset consists of student answers to 14 separate query-writing tasks, given as part of 3 separate homework assignments.
The query writing tasks have varying degrees of difficulty.
Answers are not linked to anonymous student identifiers and there is no grade information.
The IIT Bombay dataset is exclusively answers to homework assignments, so we expect generally high-quality answers due to the lack of time pressure and availability of resources for validating query correctness.

The second dataset consists of student answers to SQL questions given as part of UB CSE department's graduate database course.
The dataset consists of student answers to 2 separate query-writing tasks, each given as part of midterm exams in 2014 and 2015 respectively.
SQL queries were transcribed from hand-written exam answers, anonymized for IRB compliance and labeled with the grade the answer was given.
Quality is expected to vary, as exams are closed-book and students have limited time. 
Since 50\% of the grade is the failing criterion, it is assumed that answers conform with the task of the question if the grade is over 50\%. Also 20\% and 80\% thresholds are explored in Appendix~\ref{appendix:examGrade}.

The third dataset consists of SQL logs that capture all database activities of 11 Android phones for a period of one month.
Google+ application is selected for the study since it is one of the few applications where all users created a workload.
SQL queries collected were anonymized and some of the identified query constraints were deleted for IRB compliance~\cite{pocketdata}.

A summary of all datasets is given in Tables~\ref{tab:xdata},~\ref{tab:ub_exam}, and~\ref{tab:google_plus}.
The prose questions asked for IIT Bombay and UB Exam datatsets can be found in Table~\ref{tab:question_bombay} and~\ref{tab:local_questions}.
Not all student responses are legitimate SQL, and so queries that cannot be successfully parsed by the open-source SQL parser are ignored\footnote{
\url{https://github.com/UBOdin/jsqlparser}
}. The source code used in the experiments is also released.\footnote{ 
\url{https://github.com/UBOdin/EttuBench}
}.

In the first two datasets, the query-writing task is specific.
It is expected that student answers to a single question are written with the same task.
Thus, one would expect a good distance metric to rate answers to the same question as close and answers to different questions as distant.
Similarly, using the distance metric for clustering, one would expect to see each query cluster to uniformly include answers to the same question.

In the third dataset, PocketData-Google+, the queries are generated by the Google+ application. 
Since some of the constants are replaced with standard placeholders for IRB compliance, the number of distinct queries drops significantly. 
Since there is no information about what kind of a task a query is trying to perform, each distinct query string is inspected and manually labeled. 
Queries were labeled with one of 8 different categories: Account, Activity, Analytics, Contacts, Feed, Housekeeping, Media and Photo.

\begin{table}
\begin{center}
\begin{tabular}{ c c c c }
\toprule
	\multirow{2}{*}{Question} & Total number & Number of & Number of distinct\\
	& of queries & parsable queries &  query strings\\ \midrule
	1 & 55 & 54 & 4 \\ 
	2 & 57 & 57 & 10 \\ 
	3 & 71 & 71 & 66 \\ 
	4 & 78 & 78 & 51 \\ 
	5 & 72 & 72 & 67 \\ %\midrule
	6 & 61 & 61 & 11 \\ 
	7 & 77 & 66 & 61 \\ 
	8 & 79 & 73 & 64 \\ 
	9 & 80 & 77 & 70 \\ 
	10 & 74 & 74 & 52 \\ %\midrule
	11 & 69 & 69 & 31 \\ 
	12 & 70 & 60 & 22 \\ 
	13 & 72 & 70 & 68 \\ 
	14 & 67 & 52 & 52 \\ \bottomrule
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{Summary of IIT Bombay dataset} 
\label{tab:xdata} 
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ c c c c}
\toprule
	Year & 2014 & 2015\\ \midrule
	Total number of queries & 117 & 60\\ 
	Number of syntactically correct queries & 110 & 51\\
	Number of distinct query strings & 110 & 51\\ 
	Number of queries with score $>$ 50\% & 62 & 40\\ 
	\bottomrule
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{Summary of UB Exam dataset} 
\label{tab:ub_exam} 
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{c c c}
	\toprule
	& Pocket Dataset & Google+\\
	\midrule
	All queries & 45,090,798 & 2,340,625\\

	SELECT queries & 33,470,310 & 1,352,202\\
	
	Distinct query strings & 34,977 & 135\\
	\bottomrule
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{Summary of PocketData dataset and Google+} 
\label{tab:google_plus} 
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{ c c c c}
    \toprule
     ID & Question & ID & Question  \\
     \midrule
     1 & \parbox{2.9in}{Find course\_id and title of all the courses} & 8 & \parbox{2.9in}{Find id and title of all the courses offered in Spring 2010, which have no pre-requisite}\\ \midrule
     2 & \parbox{2.9in}{Find course\_id and title of all the courses offered by ``Comp. Sci." department.} &9 & \parbox{2.9in}{Find the ID and names of all students who have (in any year/semester) taken two courses}\\ \midrule
     3 & \parbox{2.9in}{Find course\_id, title and instructor ID for all the courses offered in Spring 2010} &  10 & \parbox{2.9in}{Find the departments (without duplicates) of courses that have the maximum credits}\\ \midrule
     4 & \parbox{2.9in}{Find id and name of all the students who have taken the course ``CS-101"}& 11 & \parbox{2.9in}{Show a list of all instructors (ID and name) along with the course\_id of courses they have taught. If they have not taught any course  show the ID and name  with null value for course\_id}\\ \midrule
     5 & \parbox{2.9in}{Find which all departments are offering courses in Spring 2010}& 12 & \parbox{2.9in}{Find IDs and names all students whose name contains the substring ``sr" ignoring case. (Hint Oracle supports the functions lower and upper)}\\ \midrule
     6 & \parbox{2.9in}{Find the course ID and titles of all courses that have more than 3 credits}&  13 & \parbox{2.9in}{Using a combination of outer join and the is null predicate but WITHOUT USING "except/minus" and "not in" find IDs and names of all students who have not enrolled in any course in Spring 2010}\\ \midrule
     7 & \parbox{2.9in}{Find, for each course, the number of distinct students who have taken the course; in case the course has not been taken by any student, the value should be 0} & 14 & \parbox{2.9in}{A course is included in your CPI calculation if you passed it, or you have failed it, and have not subsequently passed it (or in other words, a failed course is removed from CPI calculation if you have subsequently passed it). Write an SQL query that shows all tuples of the relation other than those eliminated by the above rule, and also eliminating tuples with a null value for grade}\\ \midrule
    \bottomrule 
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{Questions given IIT Bombay Dataset~\cite{chandra2015Data}}
\label{tab:question_bombay}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ c c  }
\toprule
Year & Question\\
\midrule                                                                                                                                                                                                                                                                                                                                                                                                                      
2014 & \parbox{2.8in}{How many distinct species of bird have ever been seen by the observer who saw the most birds on December 15, 2013?}                                                                                                                                                                                                                                                                                                               \\  \midrule
2015 &  \parbox{2.8in}{You are hired by a local birdwatching organization, who's database uses the Birdwatcher Schema on page 2. You are asked to design a leader board for each species of Bird. The leader board ranks Observers by the number of Sightings for Birds of the given species. Write a query that computes the set of names of all Observers who are highest ranked on at least one leader board. Assume that there is no tied rankings.} \\ 
\bottomrule
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{UB Exam dataset questions}
\label{tab:local_questions}
\end{table}

\subsection{Clustering validation measures}
\label{subsec:validation}

In addition to workload datasets, a set of measures are defined to be used for evaluating queries.
Given a set of queries labeled with tasks and an inter-query similarity metric, we aim at understanding how well the metric can (1) put queries that perform the same task close together even if they are written differently, and (2) differentiate queries that are labeled with different tasks. 

Each metric is evaluated according to how well it aligns with the ground-truth cluster labels. 
Rather than evaluating the clustering output itself, an intermediate step is evaluated: the pairwise distance matrix for the set of queries in a given workload. 
With this matrix and a labeled dataset, we can use various clustering validation measures to understand how effectively a similarity metric characterizes the partition of a set of queries. 
Specifically, clustering validation measures are used to validate the quality of a labeled dataset by estimating two quantities: (1) the degree of tightness of observations in the same label group and (2) the degree of separations between observations in different label groups. 
As a result, three clustering validation measures will be used~\cite[Chapter~17]{zaki2014data} including Average Silhouette Coefficient, BetaCV and Dunn Index as they all quantify the two qualities mentioned above in their formulations.

\tinysection{Silhouette coefficient} 
For every data point in the dataset, its silhouette coefficent is a measure of how similar it is to its own cluster in comparison to other clusters. 
In particular, the silhouette coefficient for a data point $i$ is measured as $\frac{b(i)-a(i)}{max(a(i),b(i))}$ where $a(i)$ is the average distance from $i$ to all other data points in the same cluster and $b(i)$ is the average distance from $i$ to all other data points in the closest neighboring cluster. 
The range of silhouette coefficient is from $-1$ to $1$. 
We denote $s(i)$ to represent silhouette coefficient of data point $i$. 
$s(i)$ is close to 1 when $s(i)$ is close to other data points from the same cluster more than data points from different clusters, which represents a good match. 
On the other hand, $s(i)$ which is close to $-1$ represents that the data point $i$ stayed in the wrong cluster, as it is closer to data points in different clusters than its own. Since the silhouette coefficient represents a measure of degree of goodness for each data point, to validate the effectiveness of the distance metric given a query partition, the average silhouette coefficient of all data points (all queries) in the dataset is used. 

\tinysection{BetaCV measure} 
The BetaCV measure is the ratio of the total mean of intra-cluster distance to the total mean of inter-cluster distance. The smaller the value of BetaCV, the better the similarity metric characterizes the cluster partition of queries on average.

\tinysection{Dunn Index} 
The Dunn Index is defined as the ratio between minimum distance between query pairs from different clusters and the maximum distance between query pairs from the same cluster.
In other words, this is the ratio between closest pairs of points from different clusters over the largest diameter among all clusters.
Higher values of the Dunn Index indicate better the worst-case performance of the clustering metric.
