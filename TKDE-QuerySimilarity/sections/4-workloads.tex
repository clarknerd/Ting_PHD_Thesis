%!TEX root = ../paper.tex
%In this section, we introduce \dcabench, a new benchmark for query similarity measures and we will use it to evaluate three query similarity metrics from our survey in Section~\ref{sec:experiment}.
In this section, we introduce the quality measures and workloads to evaluate three query similarity metrics and the feature engineering scheme.
% from our survey in Section~\ref{sec:experiment}.
Our goal is to evaluate how well a query similarity metric captures the task behind a query with and without regularization.
We use two types of real-world query workloads: human- and machine-generated.  
We expect the problem of query similarity to be harder on human-generated workloads, as queries generated by machines are more likely to follow a strict, rigid structural pattern.

As a source of human-generated queries, we use two different sets of student answers to database course assignments.
Many database courses include homework or exam questions where students are asked to translate prose into a precise SQL query.  
This provides us with a ground-truth source of queries with different structures that should be similar.
%The motivation for choosing database course assignments is threefold:
%(1) There is minimal bias or correlation between results, as the prose that students are given has a completely different structure from SQL, 
%(2) ground truth for the task of each query is clear: students are attempting to accomplish the stated task, and
%(3) scores (if available) also provide an even more precise metric of how close a query is to the specified task.
As machine-generated queries, we use PocketData~\cite{pocketdata} a log of 33 million queries issued by smartphone apps running on 11 phones in the wild over the course of a month.

In subsection~\ref{subsec:data}, we outline the datasets used.
% for \dcabench, as well as our methodology for transcribing student answers from text.
Then, in subsection~\ref{subsec:validation}, we outline the experimental methodology used to evaluate distance metrics, and propose a set of measures for quantitatively assessing how effective a query similarity metric is at clustering queries with similar tasks.
%Finally, in subsection~\ref{subsec:experiments}, we use \dcabench to evaluate the three distance metrics that appear in our literature survey.

%In this section, we perform a set of experiments to compare the effectiveness of our proposed query similarity metric with other metrics that has been proposed in other research works. In subsection~\ref{subsec:data}, we will briefly describe the datasets we used and preprocessing steps to clean data. In subsection~\ref{subsec:validation}, we will describe the set of validations that we will use to compare different query similarity metrics. In subsection~\ref{subsec:experiments}, we will give experimental results to compare our proposed similarity metrics with other metrics that has been used in other research works.

%With the proposed query similarity metric and a regularization step that allows us to normalize queries into a regularized format as given in Section~\ref{sec:system}, one may come up with the following questions: 
%\texttt{\begin{itemize}
%	\item Among our proposed query similarity metric and other similarity metric which has been proposed in literature as mentioned in Section~\ref{sec:background}, which query similarity metric is the best in terms of capturing the notion of query semantic?
%	\item Whether the regularization step presented in Section \ref{sec:system} can help a particular query similarity metric in terms of improving its ability to characterize query semantic?
%	\item In case that a particular query similarity metric put a query that performs one particular task into a group of queries that performs another task, can we explain why it happens?
%	\item Compare running time among different query similarity metrics and understand why one metric is faster than another?
%\end{itemize}
%These questions are our main interest in this section and they are the primary force that drives our experiments. In this section, we present a set of experiments to with the aim to answer the above questions. In subsection~\ref{subsec:data}, we will briefly describe the datasets we used and preprocessing steps to clean data. In subsection~\ref{subsec:experiments}, we will give experimental results to answer each of the aforementioned question.
%}
\subsection{Workloads}
\label{subsec:data}
%Concretely, \dcabench relies on two specific query sets: One gathered by IIT Bombay~\cite{chandra2015Data} and one gathered at
%
We use three specific query sets: Student assignments gathered by IIT Bombay~\cite{chandra2015Data}, student exams gathered at
%the University at Buffalo
our department (denoted as UB dataset in the experiments)
and released as part of this article\footnote{\url{http://odin.cse.buffalo.edu/public_data/2016-UB-Exam-Queries.zip}
%Link anonymized for double-blind review procedure.
}, and SQL query logs of the Google+ app extracted from PocketData dataset~\cite{pocketdata}.

The first dataset~\cite{chandra2015Data} consists of student answers to SQL questions given in IIT Bombay's undergraduate databases course.
The dataset consists of student answers to 14 separate query-writing tasks, given as part of 3 separate homework assignments.
The query writing tasks have varying degrees of difficulty.
Answers are not linked to anonymous student identifiers and there is no grade information.
The IIT Bombay dataset is exclusively answers to homework assignments, so we expect generally high-quality answers due to the lack of time pressure and availability of resources for validating query correctness.
%A summary of IIT Bombay dataset is given in Table~\ref{tab:xdata}. 
%The prose questions are given in Table~\ref{tab:question_bombay}.

The second dataset consists of student answers to SQL questions given as part of 
%the University at Buffalo's
our department's
graduate database course.
The dataset consists of student answers to 2 separate query-writing tasks, each given as part of midterm exams in 2014 and 2015 respectively.
SQL queries were transcribed from hand-written exam answers, anonymized for IRB compliance and labeled with the grade the answer was given.
We expect quality to vary, as exams are closed-book and students have limited time. 
%Unless indicated, we use the full dataset in our experiments.
Since 50\% of the grade is the failing criterion, we assume that answers conform with the task of the question if the grade is over 50\%. We also explore 20\% and 80\% thresholds in Appendix~\ref{appendix:examGrade}.

The third dataset consists of SQL logs that capture all database activities of 11 Android phones for a period of one month.
We selected Google+ application for our study since it is one of the few applications where all users created a workload.
SQL queries collected were anonymized and some of the identified query constraints were deleted for IRB compliance~\cite{pocketdata}.

A summary of all datasets is given in Tables~\ref{tab:xdata},~\ref{tab:ub_exam}, and~\ref{tab:google_plus}.
The prose questions asked for IIT Bombay and UB Exam datatsets can be found in Table~\ref{tab:question_bombay} and~\ref{tab:local_questions}.
%in the report released\footnote{
%\url{http://odin.cse.buffalo.edu/papers/2017/EttuQuestionReport.pdf}
%Link anonymized for double-blind review procedure.
%}.
Not all student responses are legitimate SQL, and so we ignore queries that cannot be successfully parsed by our open-source SQL parser\footnote{
\url{https://github.com/UBOdin/jsqlparser}
%Link anonymized for double-blind review procedure.
}. We also released the source code we used in the experiments\footnote{ 
\url{https://github.com/UBOdin/EttuBench}
%Link anonymized for double-blind review procedure.
}.
%The number of queries from each dataset that we use is given in the summary tables.

%For our initial evaluation using \dcabench, we ignore constant literals in the queries, replacing them with placeholders.
%A query with its constant values replaced by a placeholder is called a \textit{query skeleton}. 
%For example, the two queries \texttt{SELECT username FROM USER WHERE rank = "admin"} and \texttt{SELECT username FROM USER WHERE rank = "moderator"} share the same skeleton because they have exactly the same query structure modulo constant values. 
%Analysis using the set of query skeletons instead of original SQL queries will reduce the number of distinct queries processed and in general will produce similar results. 
%The dataset summary tables indicate how many \emph{distinct} skeletons appear for each query-writing task.  
%One may notice that the number of distinct query skeletons varies in different questions. 
%This is because the level of difficulty of the question given to students as well as the time students spent on answering questions. 
%For easy questions, we expect many virtually identical answers and a small number of skeletons (e.g., as in IIT Bombay question 1).  
%Conversely, the more complex questions asked for the local exam dataset admit a much wider range of query variants.

In the first two datasets, the query-writing task is specific.
We can expect that student answers to a single question are written with the same task.
Thus, we would expect a good distance metric to rate answers to the same question as close and answers to different questions as distant.
Similarly, using the distance metric for clustering, we would expect to see each query cluster to uniformly include answers to the same question.

In the third dataset, PocketData-Google+, the queries are generated by the Google+ application. Since some of the constants are replaced with standard placeholders for IRB compliance, the number of distinct queries drops significantly. Since there is no information about what kind of a task a query is trying to perform, we inspected and manually labeled each distinct query string. Queries were labeled with one of 8 different categories: Account, Activity, Analytics, Contacts, Feed, Housekeeping, Media and Photo.

\begin{table}
\begin{center}
\begin{tabular}{ c c c c }
\toprule
	%Question & Total & Parsable & Distinct query strings\\ \midrule
	\multirow{2}{*}{Question} & Total number & Number of & Number of distinct\\
	& of queries & parsable queries &  query strings\\ \midrule
	1 & 55 & 54 & 4 \\ 
	2 & 57 & 57 & 10 \\ 
	3 & 71 & 71 & 66 \\ 
	4 & 78 & 78 & 51 \\ 
	5 & 72 & 72 & 67 \\ %\midrule
	6 & 61 & 61 & 11 \\ 
	7 & 77 & 66 & 61 \\ 
	8 & 79 & 73 & 64 \\ 
	9 & 80 & 77 & 70 \\ 
	10 & 74 & 74 & 52 \\ %\midrule
	11 & 69 & 69 & 31 \\ 
	12 & 70 & 60 & 22 \\ 
	13 & 72 & 70 & 68 \\ 
	14 & 67 & 52 & 52 \\ \bottomrule
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{Summary of IIT Bombay dataset} 
\label{tab:xdata} 
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ c c c c}
\toprule
	Year & 2014 & 2015\\ \midrule
	Total number of queries & 117 & 60\\ 
	Number of syntactically correct queries & 110 & 51\\
	Number of distinct query strings & 110 & 51\\ 
	Number of queries with score $>$ 50\% & 62 & 40\\ 
	\bottomrule
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{Summary of UB Exam dataset} 
\label{tab:ub_exam} 
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{c c c}
	\toprule
	& Pocket Dataset & Google+\\
	\midrule
	All queries & 45,090,798 & 2,340,625\\

%	Unparsable queries & 6,366,009 & 6,366,009\\
	
%	Parsable queries & 21,486,882 & 21,486,882\\
	
	SELECT queries & 33,470,310 & 1,352,202\\
	
%	Distinct queries & 3,851 & 135\\
	Distinct query strings & 34,977 & 135\\
	\bottomrule
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{Summary of PocketData dataset and Google+} 
\label{tab:google_plus} 
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{ c c }
    \toprule
     ID & Question \\
     \midrule
     1 & \parbox{2.9in}{Find course\_id and title of all the courses}\\ \midrule
     2 & \parbox{2.9in}{Find course\_id and title of all the courses offered by ``Comp. Sci." department.}\\ \midrule
     3 & \parbox{2.9in}{Find course\_id, title and instructor ID for all the courses offered in Spring 2010}\\ \midrule
     4 & \parbox{2.9in}{Find id and name of all the students who have taken the course ``CS-101"}\\ \midrule
     5 & \parbox{2.9in}{Find which all departments are offering courses in Spring 2010}\\ \midrule
     6 & \parbox{2.9in}{Find the course ID and titles of all courses that have more than 3 credits}\\ \midrule
     7 & \parbox{2.9in}{Find, for each course, the number of distinct students who have taken the course; in case the course has not been taken by any student, the value should be 0}\\ \midrule
     8 & \parbox{2.9in}{Find id and title of all the courses offered in Spring 2010, which have no pre-requisite}\\ \midrule
     9 & \parbox{2.9in}{Find the ID and names of all students who have (in any year/semester) taken two courses}\\ \midrule
     10 & \parbox{2.9in}{Find the departments (without duplicates) of courses that have the maximum credits}\\ \midrule
     11 & \parbox{2.9in}{Show a list of all instructors (ID and name) along with the course\_id of courses they have taught. If they have not taught any course  show the ID and name  with null value for course\_id}\\ \midrule
     12 & \parbox{2.9in}{Find IDs and names all students whose name contains the substring ``sr" ignoring case. (Hint Oracle supports the functions lower and upper)}\\ \midrule
     13 & \parbox{2.9in}{Using a combination of outer join and the is null predicate but WITHOUT USING "except/minus" and "not in" find IDs and names of all students who have not enrolled in any course in Spring 2010}\\ \midrule
     14 & \parbox{2.9in}{A course is included in your CPI calculation if you passed it, or you have failed it, and have not subsequently passed it (or in other words, a failed course is removed from CPI calculation if you have subsequently passed it). Write an SQL query that shows all tuples of the relation other than those eliminated by the above rule, and also eliminating tuples with a null value for grade}\\ 
    \bottomrule 
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{Questions given IIT Bombay Dataset~\cite{chandra2015Data}}
\label{tab:question_bombay}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ c c  }
\toprule
Year & Question\\
\midrule                                                                                                                                                                                                                                                                                                                                                                                                                      
2014 & \parbox{2.8in}{How many distinct species of bird have ever been seen by the observer who saw the most birds on December 15, 2013?}                                                                                                                                                                                                                                                                                                               \\  \midrule
2015 &  \parbox{2.8in}{You are hired by a local birdwatching organization, who's database uses the Birdwatcher Schema on page 2. You are asked to design a leader board for each species of Bird. The leader board ranks Observers by the number of Sightings for Birds of the given species. Write a query that computes the set of names of all Observers who are highest ranked on at least one leader board. Assume that there is no tied rankings.} \\ 
\bottomrule
\end{tabular}
\end{center}
\vspace{-3mm}
\caption{UB Exam dataset questions}
\label{tab:local_questions}
\end{table}

\subsection{Clustering validation measures}
\label{subsec:validation}

In addition to workload datasets, we define a set of measures to be used for evaluating queries.
%as part of \dcabench.
Given a set of queries labeled with tasks and an inter-query similarity metric, we want to understand how well the metric can (1) put queries that perform the same task close together even if they are written differently, and (2) differentiate queries that are labeled with different tasks. 

We evaluate each metric according to how well it aligns with the ground-truth cluster labels. 
Rather than evaluating the clustering output itself, we evaluate an intermediate step: the pairwise distance matrix for the set of queries in a given workload. 
With this matrix and a labeled dataset, we can use various clustering validation measures to understand how effectively a similarity metric characterizes the partition of a set of queries. 
Specifically, clustering validation measures are used to validate the quality of a labeled dataset by estimating two quantities: (1) the degree of tightness of observations in the same label group and (2) the degree of separations between observations in different label groups. 
As a result, we will use three clustering validation measures~\cite[Chapter~17]{zaki2014data} including Average Silhouette Coefficient, BetaCV and Dunn Index as they all quantify the two qualities mentioned above in their formulations.
%Specifically, we will use three clustering validation measures~\cite[Chapter~17]{zaki2014data}:Average Silhouette Coefficient, BetaCV and Dunn Index.

%In other words, queries that perform similar task should be spatially close together in using a paricular metric. 

%To answer the first 3 questions, we apply different query similarity metrics on two students datasets (UB Exam dataset and IIT Bombay dataset) and obtain the pairwise distance matrix. After that, with an assumption that the set of queries has been partitioned into correct clusters, in which each cluster is labeled by the question given in assignment (or exam), we will use various clustering validation measures~\cite[Chapter~17]{zaki2014data} to evaluate the pairwise distance matrix to see whether a particular metric corrrectly characterizes the notion of query semantics. Specifically, there are three clustering validation measures that we use in our experiments: Average Silhouette Coefficient, BetaCV and Dunn Index. 

%In this experiment, we first assume that the set of queries has been partitioned into correct clusters, in which each cluster is labeled by the question given in assignment (or exam). Given a particular similarity metric, we construct a pairwise distance matrix for the set of queries and then using this distance matrix to evaluate whether this particular metric correctly characterize the notion of query semantics. In particular, we evaluate the distance matrix correctly capturing the query partition using traditional clustering validation measures~\cite[Chapter~17]{zaki2014data}.

\tinysection{Silhouette coefficient} For every data point in the dataset, its silhouette coefficent is a measure of how similar it is to its own cluster in comparison to other clusters. In particular, the silhouette coefficient for a data point $i$ is measured as $\frac{b(i)-a(i)}{max(a(i),b(i))}$ where $a(i)$ is the average distance from $i$ to all other data points in the same cluster and $b(i)$ is the average distance from $i$ to all other data points in the closest neighboring cluster. The range of silhouette coefficient is from $-1$ to $1$. We denote $s(i)$ to represent silhouette coefficient of data point $i$. $s(i)$ is close to 1 when $s(i)$ is close to other data points from the same cluster more than data points from different clusters, which represents a good match. On the other hand, $s(i)$ which is close to $-1$ represents that the data point $i$ stayed in the wrong cluster, as it is closer to data points in different clusters than its own. Since the silhouette coefficient represents a measure of degree of goodness for each data point, to validate the effectiveness of the distance metric given a query partition, we use the average silhouette coefficient of all data points (all queries) in the dataset. 

\tinysection{BetaCV measure} The BetaCV measure is the ratio of the total mean of intra-cluster distance to the total mean of inter-cluster distance. The smaller the value of BetaCV, the better the similarity metric characterizes the cluster partition of queries on average.

\tinysection{Dunn Index} The Dunn Index is defined as the ratio between minimum distance between query pairs from different clusters and the maximum distance between query pairs from the same cluster.
In other words, this is the ratio between closest pairs of points from different clusters over the largest diameter among all clusters.
Higher values of the Dunn Index indicate better the worst-case performance of the clustering metric.



%\subsection{Metrics for SQL query similarity}
%\label{subsec:validation}

%Although we have proposed a SQL query similarity metric in Section~\ref{sec:system}, there are also other query similarity metrics that have been used in other research works. In order to evaluate the effectiveness of our proposed SQL query similarity metric in terms of capturing the semantic notion of queries, we will compare it with them. In this subsection, we enumerate few query similarity metrics that we will compare in the experiments.

%Makiyama's similarity (cosine similarity of Term Frequency representation of SQL query)~\cite{makiyama2015text}: queries were first preprocessed to a normalized form and feature matrix was constructed from a query set using Term Frequency - Inverse Document Frequency representation. After that, the pairwise simiarlity matrix between queries is computed  using cosine similarity.

%Aligon's similarity~\cite{aligon2014similarity}: similarity between two SQL queries are computed using a weighted combination of group-by similarity, selection similarity and measure similarity. Although the purpose of this similarity is to place the basis for similarity between OLAP sessions, whether or not this metric can capture the semantic of SQL queries is questionable and will use to compare with our proposed metric.

%Aouiche's similarity~\cite{aouiche2006}: columns are first extracted from query as attributes for query and then the Hamming distance is used to calculate the similarity (or dissimilarity) between queries. The aim of the query similarity metric proposed in Aouiche et al's work is to found a basis for materialized view selection problem. 

%It's also worth to note that we have proposed a method for query regularization in Section~\ref{sec:system}. Therefore, it's also interesting to see whether the regularization step is beneficial when doing it as a preprocessing step before computing query similarity
