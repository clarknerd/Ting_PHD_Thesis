% !TEX root = ../paper.tex
Database access logs are used in a wide variety of settings, including evaluating database performance tuning~\cite{Bruno:2005:APD:1066157.1066184}, benchmark development~\cite{pocketdata}, database auditing~\cite{kul2016ettu}, and compliance validation~\cite{Dwork2006}.
Also, many user-centric systems utilize query logs to help users by providing recommendations and personalizing the user experience~\cite{Sapia:2000:PPQ:646109.679288, giacometti2009, yang2009, stefanidis2009you, magda2010snipsuggest, chatzopoulou2011querie}. 
As the basic unit of interaction between a database and its users, the sequence of SQL queries that a user issues effectively models the user's behavior.  Queries that are similar in structure imply that they might be issued to perform similar duties. Examining a history of the queries serviced by a database can help database administrators with tuning, or help security analysts to assess the possibility and/or extent of a security breach. However, logs from enterprise database systems are far too large to examine manually. As one example, a recent study of queries at a major US bank for a period of 19 hours found nearly 17 million SQL queries and over 60 million stored procedure execution events~\cite{kul2016ettu}. Even excluding stored procedures, it is unrealistic to expect any human to manually inspect all 17 million queries per day.

Let us consider an analyst (call her Jane) faced with the task of analyzing such a query log.
Jane might first attempt to identify some interesting query fragments and their aggregate properties.
For example, she might count how many times each table is accessed or the frequency with which different classes of join predicates occur. 
Unfortunately, such fine-grained properties lack the context to clearly communicate how the data is being used, combined, and/or manipulated. 
To see the complete context, Jane must look at entire queries.
Naively, she might look at all \textit{distinct query strings} in the log.
Even comparatively small production databases typically log hundreds or thousands of distinct query strings, making direct inspection impractical.
Furthermore, it is unclear that distinct query strings are the right level of granularity in the first place.  Consider the following example queries:

{\footnotesize
\begin{enumerate}
%%%%
\item \begin{verbatim}
SELECT name FROM user
WHERE rank IN ('adm','sup')
\end{verbatim}
%%%%
\item \begin{verbatim}
SELECT SUM(balance) FROM accounts
\end{verbatim}
%%%%
\item \begin{verbatim}
SELECT name FROM user WHERE rank = 'adm'
   UNION SELECT name FROM user
         WHERE rank = 'sup'
\end{verbatim}
%%%%
\item \begin{verbatim}
SELECT SUM(accounts.balance) FROM accounts 
   NATURAL JOIN user WHERE user.rank = 'adm'
\end{verbatim}
\end{enumerate}

}

Queries 1 and 2 are clearly distinct: Their structures differ, they reference different datasets, and perform different computations. 
The remaining queries however are less so.  
Query 3 is logically equivalent to Query 1: Both compute identical results.
Conversely, although Query 4 is distinct from Queries 1 and 2, it is conceptually similar to both and shares many structural features with each.

The exact definition of similarity may depend on Jane's exact task, the content of the log, the database schema, database records, and numerous other details, some of which may not be available to Jane immediately when she first begins analyzing the log.
It is also likely that some of this information, like the precise contents of the database or even the database schema may not even be available to Jane for reasons of privacy or security.
As a result, this type of log analysis can quickly become a tedious, time-consuming process~\cite{gatterbauer2011databases}.
An earlier work of Aligon et al.~\cite{aligon2014holistic} attempted to address this problem for OLAP operations by performing query log analysis and exploration. 
Within the scope of this article, we focus on analysis of SQL queries instead of OLAP queries.
%In this article, we lay the groundwork for a more automated approach to SQL query log exploration based on hierarchical clustering.  
In particular, we lay the groundwork for a more automated approach to SQL query log exploration based on hierarchical clustering.  
Given a hierarchical clustering of the SQL query log, Jane can manually adjust how aggressively the log is summarized.
She can select an appropriate level of granularity without a \emph{priori} needing to specify exactly what constitutes a similar query.

The primary focus of this chapter is to study the suitability of three existing query distance metrics~\cite{aouiche2006,aligon2014similarity,makiyama2015text} to be used with hierarchical algorithms for clustering query logs. All of these metrics operate on the query structure and do not rely on the availability of underlying data or schema, thus making them applicable in a wide variety of practical settings. 
The three metrics are evaluated on two types of data: Human-authored and Machine-generated.
Thus, using an appropriate similarity metric, one can cluster the queries to obtain a meaningful clustering of the query log. 
For the evaluation, three evaluation data sets are used:
\begin{enumerate}
\item a large set of student authored queries released by IIT Bombay~\cite{chandra2015Data},
\item a smaller set of student queries gathered at 
the University at Buffalo,
%our university
and released as part of this publication, and
\item SQL logs that capture all activities on 11 Android phones for a period of one month~\cite{pocketdata}.
\end{enumerate}
Student-written queries are appealing, as queries are already labeled by their ground-truth clusters --- For each question, the student is attempting to accomplish one specific stated task.
Conversely, machine-generated queries on smart phones present a conceptually easier challenge, as they produce more rigid, structured queries.
The three similarity metrics are evaluated on these data sets using three standard clustering evaluation statistics: Silhouette Coefficient, Beta CV, and Dunn Index~\cite{zaki2014data}. 

None of the similarity metrics perform as well as desired, so a pre-processing step is proposed and evaluated, in order to create more regular, uniform query representations by leveraging query equivalence rules and data partitioning operations. These rules are commonly utilized by database management systems when parsing and evaluating SQL queries.
This process significantly improves the quality of all three distance metrics.
Sources of errors will be identified and investigated in the clustering process. 
Experimental results show that our \emph{regularization} pre-processing technique consistently improves clustering for different query comparison schemes from the literature.

Concretely, the specific contributions of this chapter are:
\begin{enumerate}
\item A survey of existing SQL query similarity metrics,
\item An evaluation of these metrics on multiple query logs, and 
\item Applying query standardization techniques to improve query clustering accuracy.
\end{enumerate}

This chapter is organized as follows.
A feature engineering technique called regularization is described in Section~\ref{sec:system}.
In Section~\ref{sec:dcabench}, query workloads will be explained and a strategy for evaluating the quality of query similarity metrics will be proposed.
The evaluation is presented in Section~\ref{sec:experiment}.
Experiment results, findings and ideas to further build upon the surveyed techniques are discussed in Section~\ref{sec:discussion}, and in Section~\ref{sec:scenarios}, we explain how this work can be beneficial by giving real life examples.
