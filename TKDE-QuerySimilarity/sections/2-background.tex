% !TEX root = ../paper.tex

Analyzing query logs mostly relies on the structure of queries~\cite{Kamra2007SyntaxBased}, although their motivations are different; some methods prefer using the log as a resource to collect information to build user profiles, and the others utilize structural similarity to perform tasks like query recommendation~\cite{giacometti2009, yang2009, chatzopoulou2011querie}, performance optimization~\cite{aouiche2006},  session identification~\cite{aligon2014similarity} and workload analysis~\cite{makiyama2015text}.  A summary of these methods is given in Table~\ref{table:literaturereview}.

There are also other possible approaches; like data-centric query comparison~\cite{Mathew2010Raid}, and utilizing the \textit{access areas} of user queries by inspecting the data partition the query is interested in~\cite{nguyen2015identifying} from the \texttt{WHERE} condition.
However, these approaches are out of our scope
since we are interested in comparing and improving methods based on structural similarity; we assume that
we do not have access to the data or the statistical information about the database. 

\begin{table*}[h]
\centering
\begin{tabular}{ C{3.8cm}  C{2.9cm}  C{3.9cm}  C{1.15cm}  C{2.65cm} C{1.35cm}}
\toprule
Paper title & Motivation & Features & Feature Structure & Distance Function & Similarity Ratio \\
\midrule
Agrawal \textit{et al.} (2006)~\cite{agrawal2006context} & Q. reply importance & {\footnotesize Schema, rules} & Vector & Cosine similarity & No \\
Giacometti \textit{et al.} (2009)~\cite{giacometti2009} & Q. recommendation & {\footnotesize Difference pairs} & Set  & Difference query  & No \\[-1mm]
Yang \textit{et al.} (2009)~\cite{yang2009} & Q. recommendation & {\footnotesize Selection/join, projection} & Graph & \parbox{2.6cm}{\footnotesize \center Jaccard coefficient\\[-0.5mm] on the graph edges} & No \\
Stefanidis \textit{et al.} (2009)~\cite{stefanidis2009you} & Data Recommendation & {\footnotesize Inner product of two queries} & Vector & - &  No \\
Khoussainova \textit{et al.} (2010)~\cite{magda2010snipsuggest} & Q. recommendation & {\footnotesize Popularity of each query object} & Graph & -  & No \\[-1mm]
Chatzopoulou \textit{et al.} (2011)~\cite{chatzopoulou2011querie} & Q. recommendation & {\footnotesize Syntactic element frequency} & Vector & \parbox{2.65cm}{\footnotesize \center Jaccard coefficient\\[-0.5mm]and cosine similarity} & No  \\
Aouiche \textit{et al.} (2006)~\cite{aouiche2006} & View selection & {\footnotesize Selection/join, group-by} & Vector & Hamming distance  & Yes \\
Aligon \textit{et al.} (2014)~\cite{aligon2014similarity} & Session similarity & {\footnotesize Selection/join, projection, group-by} & 3 Sets & Jaccard coefficient   & Yes \\
Makiyama \textit{et al.} (2016)~\cite{makiyama2015text} & Workload analysis & {\footnotesize Term frequency of projection, selection/join, from, group-by and order-by} & Vector & Cosine similarity  & Yes \\
\bottomrule
\end{tabular}
\vspace*{-2mm}
\caption{SQL query similarity literature review}
\label{table:literaturereview}
\end{table*}

Agrawal \textit{et al.}~\cite{agrawal2006context} aim to rank the tuples returned by the SQL query based on the context.
They create a ruleset for contexts and evaluate the result of queries that belongs to the context according to the ruleset.
They capture context and query as feature vectors and capture similarity through cosine distance between the vectors.

Chatzopoulou \textit{et al.}~\cite{chatzopoulou2011querie} aim to assist non-expert users of scientific databases by tracking their querying behavior and generating personalized query recommendations.
They deconstruct an SQL query into a bag of \textit{fragments}.
Each distinct fragment is a feature, with a weight assigned to it indicating its importance.
Each feature has two types of importance: (1) within the query and (2) for the overall workload.
Similarity is defined upon common vector-based measures such as cosine similarity.
A summarization/user profile for this approach is just a sum over all single query feature vectors that belong to their workload.

Yang \textit{et al.}~\cite{yang2009}, on the other hand, build a graph following
the query log by connecting associations of table attributes from the input and output of queries which are then used to compute the likelihood of an attribute appearing in a query with a similarity function like Jaccard coefficient.
Their aim is again to assist users in writing SQL queries by analyzing query logs. Giacometti \textit{et al.}~\cite{giacometti2009}, similarly, aim to make recommendations on the discoveries made in the previous sessions for users to spend less time on investigating similar information.
They introduce \textit{difference pairs} in order to measure the relevance of the previous discoveries.
Difference pairs are essentially the result columns that is not included in the other return results; hence the method depends on having access to the data.
Stefanidis \textit{et al.}~\cite{stefanidis2009you} takes a different approach, and instead of recommending candidate queries, they recommend tuples that may be of interest to the user.
By doing so, the users may decide to change the selection criteria of their queries in order to include these results.

Sapia~\cite{Sapia:2000:PPQ:646109.679288} creates a model that learns query templates to prefetch data in OLAP systems based on the user's past activity. SnipSuggest~\cite{magda2010snipsuggest}, on the other hand, is a context-aware SQL-autocomplete system that helps database users to write SQL queries by suggesting SQL snippets.
In particular, it assigns a probability score to each subtree of a query based on the subtree's frequency in a query log.
These probabilities are used to discover the most likely subtree that a user is attempting to construct, at interactive speeds.

Although these methods~\cite{agrawal2006context, chatzopoulou2011querie, yang2009, giacometti2009, stefanidis2009you, magda2010snipsuggest} utilize query similarity one way or other to achieve their purpose, they don't directly offer a way to compare query similarity. 
We aim to summarize the log and the most practical way to describe a query log is to group similar queries together so that we can provide summaries of these groups to the users.
For this purpose, we need to be able to measure pairwise similarity between each query, hence we need a metric that can do so. As shown in Table~\ref{table:literaturereview}, this condition is only satisfied by \cite{aouiche2006, aligon2014similarity, makiyama2015text}.

Aouiche \textit{et al.}~\cite{aouiche2006} is the first work we encountered that proposes a pairwise similarity metric between two SQL queries although it is not the aim of their work.
They aim to optimize view selection in warehouses by the queries posed to the system.
They consider the \textit{selection}, \textit{joins} and \textit{group-by} items in the query to create vectors and use Hamming Distance to measure how similar two queries are.
While creating the vector, it doesn't matter if an item appears more than once or where the item is.
They cluster similar queries that creates a workload on the system and base their view creation strategy in the system on the clustering result. 

Aligon \textit{et al.}~\cite{aligon2014similarity} study various approaches to defining a similarity function to compare OLAP sessions.
They focus on comparing session similarity while also performing a survey on query similarity metrics.
They identify \textit{selection} and \textit{join} items as the most relevant components in a query followed by the \textit{group by} set.
Inspired by the findings, they propose their own query similarity metric which considers \textit{projection}, \textit{group-by}, \textit{selection-join} items for queries issued on OLAP datacubes. 
OLAP datacubes are multidimensional models, and they have hierarchy levels for the same attributes. Aligon \textit{et al.}~\cite{aligon2014similarity} measure the distance between the attributes on different hierarchy levels, and compute the set similarity for \textit{projection}, \textit{group-by}, and \textit{selection-join} sets individually when comparing two queries.
In our experiments, since we do not consider the hierarchy levels in an OLAP system but focus on databases, we consider all queries are on the same level in the schema to adjust the formulas presented in the paper. 
Namely, we compute set similarity of \textit{projection}, \textit{group-by}, \textit{selection-join} sets of two queries with Jaccard coefficient. 
Also, Aligon \textit{et al.}~\cite{aligon2014similarity} provide the flexibility to adjust weights of the three feature sets based on the domain needs. We explore how the clustering quality is affected with various weightings in Appendix~\ref{appendix:aligon}.

Makiyama \textit{et al.}~\cite{makiyama2015text} approach query log analysis with the goal of analyzing a system's workload, and they provide a set of experiments on Sloan Digital Sky Survey (SDSS) dataset.
They extract the terms in \textit{selection}, \textit{joins}, \textit{projection}, \textit{from}, \textit{group-by} and \textit{order-by} items separately and record their appearance frequency.
They create a feature vector using the frequency of these terms which they use to calculate the pairwise similarity of queries with cosine similarity.
Instead of clustering, they perform the workload analysis with Self-Organizing Maps (SOM).

To further illustrate how the three structural metrics~\cite{aouiche2006, aligon2014similarity, makiyama2015text} work, we show the feature representations for the following query for each method in Table~\ref{tab:features}.
{\footnotesize
\begin{verbatim}
SELECT u.username, u.yearenrolled
FROM user u, accounts a
WHERE u.id = a.userid
  AND a.balance > 1000
  AND u.id > 20050001
GROUP BY u.yearenrolled
ORDER BY u.yearenrolled
\end{verbatim}
}

\begin{table}[]
\centering

\begin{tabular}{C{2cm}  l}
\toprule

        Paper title & \multicolumn{1}{c}{Extracted Feature Vector}                                                                                                                                                                                                                                                                                                                                               \\ \midrule
Aouiche \textit{et al.} (2006)~\cite{aouiche2006}  & \begin{tabular}[c]{@{}l@{}}\{`u.id', `a.userid', `a.balance',  `u.yearenrolled'\}\end{tabular}                                                                                                                                                                                                                                                                                                   \\ \midrule
Aligon \textit{et al.} (2014)~\cite{aligon2014similarity}   & \begin{tabular}[c]{@{}l@{}}\{`u.username`, `u.yearenrolled'\}\\ \{`u.id', `a.userid', `a.balance'\}\\ \{`u.yearenrolled'\}\end{tabular}                                                                                                                                                                                                                                                                \\ \midrule
Makiyama \textit{et al.} (2016)~\cite{makiyama2015text} & \begin{tabular}[c]{@{}l@{}}\{`SELECT\_u.username' \textrightarrow 1,\\  `SELECT\_u.yearenrolled' \textrightarrow 1,\\   `FROM\_user \textrightarrow 1', `FROM\_accounts' \textrightarrow 1,\\   `WHERE\_u.id' \textrightarrow 2, `WHERE\_a.userid' \textrightarrow 1,\\ `WHERE\_a.balance' \textrightarrow 1,\\   `GROUPBY\_u.yearenrolled' \textrightarrow 1,\\   `ORDERBY\_u.yearenrolled' \textrightarrow 1\}\end{tabular} \\ 
\bottomrule
\end{tabular}
\vspace*{-2mm}
\caption{Representation of three similarity metrics}
\label{tab:features}
\end{table}

In the next section, we propose a generalized feature engineering scheme for query comparison methods to improve the clustering quality.
Our work evaluates the performance of the three methods~\cite{aouiche2006, aligon2014similarity, makiyama2015text} that directly describe a pairwise similarity metric in Section~\ref{sec:dcabench} due to the lack of performance evaluation for the query similarity metrics in the given studies.
We also show that our feature engineering scheme improves the clustering quality with both statistical and empirical methods.